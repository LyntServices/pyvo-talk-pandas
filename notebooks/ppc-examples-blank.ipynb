{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world data analysis example: PPC Campaign Performance\n",
    "\n",
    "In the following example, we will load and analyze a generated set of data. The dataset is almost in the same format as could be obtained from AdWords using its reporting API, but the data itself is completely generated and any similarities with any existing AdWords Account is purely coincidental.\n",
    "\n",
    "Let's dive right in!\n",
    "\n",
    "We have to start by importing the `pandas` library. All the examples in [the official pandas documentation](http://pandas.pydata.org/pandas-docs/stable/) import the library under the `pd` alias. Futhermore, [the official NumPy documentation] also uses an alias: `np`. We'll follow these conventions and import both libraries using these aliases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working in Jupyter notebooks (especially when presenting), it might be a good idea to set the maximum number of rows displayed when printing `DataFrame`s and `Series`es. To do so, set `display.max_rows` and `display.max_seq_items` options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_seq_items', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your first table\n",
    "\n",
    "We can now proceed by creating your first table. We can initialize the `DataFrame` using:\n",
    "\n",
    "* A list of dictionaries, each dictionary will represent one row and dictionary keys will be mapped to columns. Please note that order of columns might not be preserved unless you use `OrderedDict`s, even in Python 3.6! Or, you can set the optional `columns` argument and they will be ordered accordingly.\n",
    "* A list of tuples/lists, each tuple/list wil represent one row. You can specify the optional `columns` argument to specify number of columns.\n",
    "* A generator yielding any of the above.\n",
    "* A dictionary of columns, key will be mapped to columns and each value should contain a list of values. As in the first method, you might need to use `OrderedDict` in order to preserve column order.\n",
    "* ... and a couple of other methods which are well-described in [the documentation of DataFrame constructor](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html).\n",
    "\n",
    "Let's try some of these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = pd.DataFrame([\n",
    "    {'name': 'Arthur Dent', 'homeworld': 'Earth', 'bill': 8.45},\n",
    "    {'name': 'Ford Prefect', 'homeworld': 'Betelgeuse Five', 'bill': 85.9},\n",
    "    {'name': 'Tricia McMillan', 'homeworld': 'Earth', 'bill': 10.2},\n",
    "])\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "restaurant = pd.DataFrame([\n",
    "    OrderedDict([('name', 'Arthur Dent'), ('homeworld', 'Earth'), ('bill', 8.45)]),\n",
    "    OrderedDict([('name', 'Ford Prefect'), ('homeworld', 'Betelgeuse Five'), ('bill', 85.9)]),\n",
    "    OrderedDict([('name', 'Tricia McMillan'), ('homeworld', 'Earth'), ('bill', 10.2)]),\n",
    "])\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = pd.DataFrame([\n",
    "    {'name': 'Arthur Dent', 'homeworld': 'Earth', 'bill': 8.45},\n",
    "    {'name': 'Ford Prefect', 'homeworld': 'Betelgeuse Five', 'bill': 85.9},\n",
    "    {'name': 'Tricia McMillan', 'homeworld': 'Earth', 'bill': 10.2},\n",
    "], columns=('name', 'homeworld', 'bill'))\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = pd.DataFrame([\n",
    "    ('Arthur Dent', 'Earth', 8.45),\n",
    "    ('Ford Prefect', 'Betelgeuse Five', 85.9),\n",
    "    ('Tricia McMillan', 'Earth', 10.2),\n",
    "], columns=('name', 'homeworld', 'bill'))\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = pd.DataFrame(OrderedDict([\n",
    "    ('name', ['Arthur Dent', 'Ford Prefect', 'Tricia McMillan']),\n",
    "    ('homeworld', ['Earth', 'Betelgeuse Five', 'Earth']),\n",
    "    ('bill', [8.45, 85.9, 10.2]),\n",
    "]))\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from an XLSX file\n",
    "\n",
    "Usually, you'll need to load some data that are already stored in some other format. pandas contains support for loading from various formats: CSV, XLS, XLSX, JSON, HDF5 and a [few other formats](http://pandas.pydata.org/pandas-docs/stable/io.html). To read an XLSX file, you'll need the `xlrd` library installed. pandas can read a single sheet specified by the `sheet_name` parameter, or it can read everything and will return a dictionary of data frames (unless there is only one sheet - in that case, it will return just the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance = pd.read_excel(\n",
    "    '../data/data_ad_group_performance.xlsx'\n",
    ")\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table we have just loaded contains daily performance of Ad Groups for 18 weeks. For every Ad Group, there are 18 * 7 = 126 rows with performance metrics, each for a single day.\n",
    "\n",
    "The table contains the following columns:\n",
    "\n",
    "* `CampaignId`: Internal ID of Campaign in AdWords.\n",
    "* `CampaignName`: Name of the Campaign.\n",
    "* `AdGroupId`: Internal ID of Ad Group in AdWords.\n",
    "* `AdGroupName`: Name of the Ad Group.\n",
    "* `Date`: Parsed date. pandas recognizes dates stored in XLSX files.\n",
    "* `Impressions`: How many times any ad from the Ad Group was served and displayed on that days.\n",
    "* `Clicks`: How many people clicked on the Ad and therefore visited our website.\n",
    "* `Cost`: How much these clicks cost. Remember, PPC is Pay-Per-Click.\n",
    "* `Conversions`: Number of conversions, e.g. how many people, who clicked on an ad and visited our website, actually bought anything.\n",
    "* `ConversionsValue`: Total revenue from all of these purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection\n",
    "\n",
    "We can access individual columns using their name using the indexing operator (like accessing an item of `dict`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['CampaignName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['Impressions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column is actualy a named series. Note that each column has its own `dtype`.\n",
    "\n",
    "We can also \"select\" columns if you pass a list of their names to the indexing operator. We will get another table with subset of columns (you can call it projection, if you are into relational algebra):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance[\n",
    "    ['CampaignName', 'AdGroupName', 'Impressions']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the rows using their value in index. Since we didn't tell pandas anything about the index of the table, it generated a default 0-based numeric index. This means that we can access the rows like elements in the array using the special `loc` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance.loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass an array, we can also get multiple rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance.loc[\n",
    "    [5, 6, 7, 8, 15, 25]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use slicing to get the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can also pass a column name to get only a specific cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance.loc[4, 'Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other ways to access the columns and rows, se the documentation chapter on [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html) to get more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Selecting rows by their index is not very useful. We might want to get specific rows matching our own condition. Luckily, pandas has it covered: we can pass a series of `bool`s to the indexing operator. The series must have the same size as there are rows in the `DataFrame`. We can get such series by simply taking one of the columns in the table and comparing it to value (or other series). `pandas.Series` supports all kinds of operators: standard math (`+` `-` `*` `/` `**` `%`), relational operators (`>` `>=` `<` `<=` `==` `!=`) and logical operators (`&` `|` `~`). Each of these operators are applied on every item of the series and a new series with results is returned.\n",
    "\n",
    "So, let's assume we would like to get rows where the number of impressions is less than 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance[\n",
    "    ad_group_performance['Impressions'] < 10\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple series using the `&` and `|` operators. To find rows where number of impressions is greater than 100 and number of conversions is 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance[\n",
    "    (ad_group_performance['Impressions'] > 100) &\n",
    "    (ad_group_performance['Conversions'] == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.Series` also supports the `~` unary operator for negation. To get rows with number of impressions greater than 100 and conversions not equal to 0 (pay attention to the tiny snake in front of the second parentheses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance[\n",
    "    (ad_group_performance['Impressions'] > 100) &\n",
    "    ~(ad_group_performance['Conversions'] == 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations\n",
    "\n",
    "We'll continue our tour by computing a few metrics that are common in the PPC world. They are described in the slides.\n",
    "\n",
    "**→ Switch to the slides and continue on slide 31 if you are interested.**\n",
    "\n",
    "We can add a new columns to the table just by assigning them. We can assign either a new series, or a constant value - it will be repeated in the every row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['TheAnswer'] = 42\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute CTR by taking the `Clicks` column and dividing it by `Impressions` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['CTR'] = (\n",
    "    ad_group_performance['Clicks'] /\n",
    "    ad_group_performance['Impressions']\n",
    ")\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not happy with any of the columns, or if we don't need it anymore (we know The Answer), we can delete it using the `drop` method of `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance.drop(columns=['TheAnswer'])\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column is still there! That's because many functions and methods in `pandas` returns a new instance of `DataFrame` and keeps the original instance intact. Don't worry, it does it's best not to copy values when it's not necessary. To modify the instance, re-assign the variable like this:\n",
    "\n",
    "```python\n",
    "ad_group_performance = ad_group_performance.drop(columns=['TheAnswer'])\n",
    "```\n",
    "\n",
    "Or, more conviniently, most of the methods supports the `inplace` argument, which will tell pandas to modify the original instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance.drop(\n",
    "    columns=['TheAnswer'], inplace=True\n",
    ")\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the CPC and Average Conversion Value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['CPC'] = (\n",
    "    ad_group_performance['Cost'] /\n",
    "    ad_group_performance['Impressions']\n",
    ")\n",
    "\n",
    "ad_group_performance['AvgConversionValue'] = (\n",
    "    ad_group_performance['ConversionsValue'] /\n",
    "    ad_group_performance['Conversions']\n",
    ")\n",
    "\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a few `NaN` values in the `AvgConversionValue` column. That's because the `Conversions`, which is used as divisor, is zero. pandas does not raise `ZeroDivisionError` in this case and replace the value with `NaN`. We can check for `NaN`s, as well as for `None`, using the `pandas.isnull` (alias of `pandas.isnan`) function. To get all rows where `AvgConversionValue` is `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance[\n",
    "    pd.isnull(ad_group_performance['AvgConversionValue'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also be interested in descriptive statistics of individual columns, such as:\n",
    "\n",
    "* What is the total number of clicks we received?\n",
    "* What is the median value of number of conversions?\n",
    "* What is the minumum number of impressions we got?\n",
    "\n",
    "pandas can answer all of these questions (and many more) easily - see the documentation on [computations and descriptive statistics](https://pandas.pydata.org/pandas-docs/stable/api.html#computations-descriptive-stats) for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['Clicks'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['Conversions'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['Impressions'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the minumum number of impressions is 0 - we might have stopped some ad groups, but it can also indicate some larger problem, for instance we might have ran out of credit in our wallet. Let's investigate!\n",
    "\n",
    "We start by searching for rows with zero impressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_groups_zero_impr = ad_group_performance[\n",
    "    ad_group_performance['Impressions'] == 0\n",
    "]\n",
    "ad_groups_zero_impr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, dates 2018-03-17 and 2018-03-18 repeats quite often, but there are 1398 rows and it might be difficult to search them manually. Let's group the values by campaign and date and see how many ad groups without impressions there are in each campaign and for each day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "Grouping can be done using [the `DataFrame.groupby` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_groups_zero_impr.groupby(\n",
    "    ['CampaignName', 'Date']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that is not very useful. pandas will defer the actual grouping operation until you perform any action with it. You can list the groups, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_groups_zero_impr.groupby(\n",
    "    ['CampaignName', 'Date']\n",
    ").groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see that there are 6 groups, each with 2 dates: 2018-03-17 and 2018-03-18. This confirms our suspicion that this is an error caused by insufficient credit in the wallet, but it is still quite hard to read. Let's get the number of values for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_campaign_date = ad_groups_zero_impr.groupby(\n",
    "    ['CampaignName', 'Date']\n",
    ").count()\n",
    "counts_by_campaign_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't specify any column, pandas simply computed the counts for each of the columns. The aggregation counts only non-`Nan` and non-`None` values, therefore there are zeros in the `CTR`, `CPC` and `AvgConversionValue` columns.\n",
    "\n",
    "You can also se that the first two columns, `CampaignName` and `Date`, are printed in bold and their names are not on the same line. This means that pandas created a hierachical index from the values. We can access any group on the first level by passing that group to the `loc` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_campaign_date.loc['Sport']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access a specific row by passing a tuple with values from `CampaignName` and `Date`, in that order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_campaign_date.loc[('Sport', '2018-03-18')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, you can access any sub-group on any level, just pass a tuple containing a path to that group.\n",
    "\n",
    "You can also let pandas produce a table without the hierarchical index (like a GROUP BY clause in SQL) if you set the `as_index` argument to `False`. pandas will generate a numerical zero-based index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_groups_zero_impr.groupby(\n",
    "    ['CampaignName', 'Date'],\n",
    "    as_index=False\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the metrics only for one of the columns, just pick any of the columns without `NaN`, for instance `AdGroupId` and compute statistics for that column (pandas will return a series with values only with that column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_groups_zero_impr.groupby(\n",
    "    ['CampaignName', 'Date']\n",
    ")['CampaignId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take advantage of grouping and answer another question: which day of week performs the best?\n",
    "\n",
    "We need to extract the day of week from the date and then group by that column. `pandas.Series` has a bunch of [methods for working with dates](https://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties), so it is quite straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance['DayOfWeek'] = (\n",
    "    ad_group_performance['Date'].dt.dayofweek\n",
    ")\n",
    "ad_group_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 is Monday and 6 is Sunday, easy. We can now group by the column and compute the statistics. However, we will need to aggregate multiple columns at once. This is a good job for the `agg` method. It takes a dictionary, where keys equals to columns which will be used for the aggregations, and values are either `str`s with aggregation method to evaluate (such as `sum`, `count`, `min`, `mean`), or custom functions.\n",
    "\n",
    "When using a custom function, it will receive a single argument: a `Series` with chunk of data to aggregate. We will try it later on, let's use the already defined methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_performance = ad_group_performance.groupby(\n",
    "    ['DayOfWeek']\n",
    ").agg({\n",
    "    'Impressions': 'sum',\n",
    "    'Clicks': 'sum',\n",
    "    'Cost': 'sum',\n",
    "    'Conversions': 'sum',\n",
    "    'ConversionsValue': 'sum',\n",
    "})\n",
    "daily_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So many numbers! Let's sort it by `ConversionsValue`, which is our revenue from the advertisment (note the `inplace=True` argument):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_performance.sort_values(\n",
    "    by='ConversionsValue',\n",
    "    ascending=False,\n",
    "    inplace=True\n",
    ")\n",
    "daily_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Wednesday is by far our most profitable day of week. This is dependent on your type of business, people generally shop less during the weekends.\n",
    "\n",
    "But that is for the whole account. What if we wanted to examine individual campaings over the week? pandas supports pivoting to do exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting\n",
    "\n",
    "Pivoting is an operation that transposes rows to columns based on a value in one or more columns - columns will be named after the values. We can perform this operation either on rows, or on hierarchical index, but we have to use a different method.\n",
    "\n",
    "First of all, we will show how to perform pivoting on columns. We use the `DataFrame.unstack` method, we have to pass a level of the hierarchical index on which to operate - either number, name (such as `DayOfWeek`) or `-1` to operate on the last level (the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance = ad_group_performance.groupby(\n",
    "    ['CampaignName', 'DayOfWeek']\n",
    ").agg({\n",
    "    'Impressions': 'sum',\n",
    "    'Clicks': 'sum',\n",
    "    'Cost': 'sum',\n",
    "    'Conversions': 'sum',\n",
    "    'ConversionsValue': 'sum',\n",
    "})\n",
    "campaign_weekday_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance = (\n",
    "    campaign_weekday_performance.unstack(\n",
    "        level='DayOfWeek'\n",
    "    )\n",
    ")\n",
    "campaign_weekday_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value we would like to use for pivoting is not in an index, but in any of the columns of the table (for instance if we passed `as_index=False` to the `groupby` method), we have to use the `DataFrame.pivot` method. The method requires at least two parameters: `index` which will tell pandas what colum shall be used to identify rows, and `columns` which will specify column name(s) whose values will be used to create new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance = ad_group_performance.groupby(\n",
    "    ['CampaignName', 'DayOfWeek'],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Impressions': 'sum',\n",
    "    'Clicks': 'sum',\n",
    "    'Cost': 'sum',\n",
    "    'Conversions': 'sum',\n",
    "    'ConversionsValue': 'sum',\n",
    "})\n",
    "campaign_weekday_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance = (\n",
    "    campaign_weekday_performance.pivot(\n",
    "        index='CampaignName',\n",
    "        columns='DayOfWeek'\n",
    "    )\n",
    ")\n",
    "campaign_weekday_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas actually created a hierarchical columns. Column name is on the first level and `DayOfWeek` on the second one. Just like with hierarchical indexes, we can access specific group or specific column by passing a value or tuple with path to the indexing operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance['Impressions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance[('Impressions', 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another useful function for pivoting - pivot_table. Pivot function needs to aggregate data before pivoting and doesn't allow to work with duplicate column values. With pivot_table function you can aggregate data in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_weekday_performance = ad_group_performance.pivot_table(index=['CampaignName', 'DayOfWeek'],\n",
    "                                 values=['Impressions', 'Clicks', 'Cost', 'Conversions', 'ConversionsValue'],\n",
    "                                 aggfunc=np.sum)\n",
    "campaign_weekday_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Tables\n",
    "\n",
    "Until now, we worked with a single table. In practice, we often have multiple tables that contains different views on the data and we need to join them together. In AdWords, such example is the Quality Score metric. That metric is available only on the keyword-level reports, but we might want to aggregate it's value and see it on the Ad Group or even Campaign level. This will enable us to quickly find Ad Groups or Campaigns where we need to focus on the keywords and their quality.\n",
    "\n",
    "We will load another table that contains quality scores on the keyword-level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_qs = pd.read_excel(\n",
    "    '../data/data_keywords_quality_score.xlsx'\n",
    ")\n",
    "keywords_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table contains a row for each keyword in each Ad Group. It contains two metrics: `Impressions` and `QualityScore`. We would like to compute the aggregated `QualityScore` on the Ad Group level. It would be a mistake to simply calculate the mean over all keywords in an Ad Group - keywords that are rarely searched and has low quality score is usually not a big deal, but keywords with many impressions and low quality score should be fixed. Therefore, we need to calculate weighted average with number of impressions as weight.\n",
    "\n",
    "To accomplish this task, we can aggregate the values using the [`numpy.average` function](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.average.html), which allows us to set the `weights` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(chunk):\n",
    "    return np.average(\n",
    "        chunk,\n",
    "        weights=keywords_qs.loc[chunk.index, 'Impressions']\n",
    "    )\n",
    "\n",
    "ad_group_qs = keywords_qs.groupby('AdGroupId').agg({\n",
    "    'QualityScore': weighted_average\n",
    "})\n",
    "\n",
    "### Can be also written using lambda function:\n",
    "ad_group_qs = keywords_qs.groupby('AdGroupId').agg({\n",
    "    'QualityScore': \\\n",
    "        lambda chunk: np.average(\n",
    "            chunk,\n",
    "            weights=keywords_qs.loc[chunk.index, 'Impressions']\n",
    "        )\n",
    "})\n",
    "###\n",
    "\n",
    "ad_group_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are few differences, but it would be useful to see the data in context with other metrics. Let's join the tables!\n",
    "\n",
    "Before we begin, we need to aggregate the `ad_group_performance` table on the `AdGroupId` level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance_sum = ad_group_performance.groupby(\n",
    "    'AdGroupId',\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'CampaignId': 'first',\n",
    "    'CampaignName': 'first',\n",
    "    'AdGroupName': 'first',\n",
    "    'Impressions': 'sum',\n",
    "    'Clicks': 'sum',\n",
    "    'Cost': 'sum',\n",
    "    'Conversions': 'sum',\n",
    "    'ConversionsValue': 'sum',\n",
    "})\n",
    "ad_group_performance_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join itself is accomplished using [the `DataFrame.merge` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html#pandas.DataFrame.merge). We give it two tables, `left` (that's the table instance on which the method is called) and `right`, set the join type (same as in SQL: `left`, `right`, `inner`, `outer` -- there are helpful diagrams in [the documentation on merging and joining](https://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra) and in the slides - **→ see slides 33 - 36**), and the join columns: it can be a set of columns which are in both tables, or we can set different columns in both tables (we need to set the same number of columns, of course). We can even order pandas to use indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance_qs = ad_group_performance_sum.merge(\n",
    "    right=ad_group_qs,\n",
    "    left_on='AdGroupId',\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "ad_group_performance_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Now that we have successfully joined tables, we might want to save the results and give them to somebody else for further processing. We could share this notebook, but we would need to distribute all the data, the recipient will need Python with pandas, Jupyter installed... It is just easier for everyone to save it to XLSX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_group_performance_qs.to_excel(\n",
    "    '../output/out_ad_group_performance_qs.xlsx',\n",
    "    sheet_name='Ad Groups with QS'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even write multiple sheets to a single file, we just need to create an instance of `pandas.ExcelWriter` in advance, pass it to `to_excel` and then call `save` on the writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\n",
    "    '../output/out_all_relevant_tables.xlsx'\n",
    ")\n",
    "ad_group_performance_qs.to_excel(\n",
    "    writer, sheet_name='Ad Groups with QS'\n",
    ")\n",
    "campaign_weekday_performance.to_excel(\n",
    "    writer, sheet_name='Campaigns on Weekdays'\n",
    ")\n",
    "daily_performance.to_excel(\n",
    "    writer, sheet_name='Account Daily Perf'\n",
    ")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [documentation for `DataFrame.to_excel`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel) to learn more about saving to XLSX files.\n",
    "\n",
    "We might want to output the data to database. pandas uses the great SQLAlchemy library under the hood, so it supports MySQL, PostgreSQL, Microsoft SQL Server and several other databases. We just need to [initialize the database connection engine)](https://docs.sqlalchemy.org/en/latest/core/engines.html) and then call `DataFrame.to_sql`. pandas and SQLAlchemy will handle creating the table automatically. Let's output the latest table to MySQL (remember to set the `charset=utf8` in the connection string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "connection = create_engine(\n",
    "    'mysql://user:***@localhost/pyvo_pandas?charset=utf8'\n",
    ")\n",
    "ad_group_performance_qs.to_sql(\n",
    "    name='ad_group_performance_qs',\n",
    "    con=connection,\n",
    "    if_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have gone through construction of `pandas.DataFrame`s from data in Python, loading the AdWords Ad Group performance data from an XLSX file, accessing rows and columns, filtering, computing new columns, grouping, sorting and pivoting. In the end, we demonstrated joining two tables and saving to an XLS file and SQL database.\n",
    "\n",
    "The pandas library provides many other possibilities and functions that were not mentioned in this turorial. Additionally, we didn't cover visualization of the data, which is another important step during data analysis. If you are interested in learning more, there are several good sources where to start:\n",
    "\n",
    "* [The official pandas documentation](https://pandas.pydata.org/pandas-docs/stable/index.html), which was heavily referred to during the tutorial.\n",
    "* [List of pandas tutorials in the documentation](https://pandas.pydata.org/pandas-docs/stable/tutorials.html) - I can recommend the great [Pandas cookbook by Julia Evans](https://github.com/jvns/pandas-cookbook).\n",
    "* [Data Analysis with Pandas and Python on Udemy](https://www.udemy.com/data-analysis-with-pandas/) (paid course).\n",
    "* [Learning pandas - Second Edition by Michael Heydt](https://www.packtpub.com/big-data-and-business-intelligence/learning-pandas-second-edition).\n",
    "\n",
    "It is also a good idea to visit [the list of PyData projects](https://pydata.org/downloads.html) and [list of projects in the pandas Ecosystem](https://pandas.pydata.org/pandas-docs/stable/ecosystem.html) to see how pandas fits into the data science stack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
